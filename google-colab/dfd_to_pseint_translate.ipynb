{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Entrenamiento y evaluacion del modelo"
      ],
      "metadata": {
        "id": "XbZl3rpJ7AOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ejecutar el primer entrenamiento se debe cargar el dataset.csv y ejecutar el codigo, cada entrenamiento que genere mejores scores que el entrenamiento anterior se guardara en la cuenta de google drvie, en una carpeta llamada \"model_saved\" y para proximos entrenamientos buscara cargar un modelo desde esta carpeta si existe"
      ],
      "metadata": {
        "id": "_9Yy-Gjv9OOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variables importantes:\n",
        "num_epochs = 10\n",
        "**#determina la cantidad de epocas que se va a realizar el entrenamiento**\n",
        "if avg_loss <= 0.04: **#en caso de que antes de terminar la cantidad de epocas definida se alcance un avg_loss deseado, se puede parar el entrenamiento**\n"
      ],
      "metadata": {
        "id": "5jqkMrb59vsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#instalaciones previas"
      ],
      "metadata": {
        "id": "xGbZO2dR-tNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "para la generacion de las metricas de calidad"
      ],
      "metadata": {
        "id": "04-xcgyq-yy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge"
      ],
      "metadata": {
        "id": "a_kHwG_L-vIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
        "from rouge import Rouge\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "\n",
        "class CodeTranslationDataset(Dataset):\n",
        "    def __init__(self, data_pairs, tokenizer, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Pre-procesamiento de todos los datos de una vez\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "\n",
        "        for input, target in data_pairs:\n",
        "            # Limpieza básica\n",
        "            input = input.strip('\"')\n",
        "            target = target.strip('\"')\n",
        "            # target = target.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
        "\n",
        "            self.inputs.append(input)\n",
        "            self.targets.append(target)\n",
        "\n",
        "        # Tokenizar todos los ejemplos de una vez (más eficiente)\n",
        "        self.tokenized_inputs = self.tokenizer(\n",
        "            self.inputs,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        self.tokenized_targets = self.tokenizer(\n",
        "            self.targets,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.tokenized_inputs['input_ids'][idx],\n",
        "            'attention_mask': self.tokenized_inputs['attention_mask'][idx],\n",
        "            'labels': self.tokenized_targets['input_ids'][idx]\n",
        "        }\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    \"\"\"Carga el dataset usando pandas directamente\"\"\"\n",
        "    try:\n",
        "        # Primero intentamos con UTF-8, la codificación más común\n",
        "        df = pd.read_csv(file_path, header=None, encoding='utf-8')\n",
        "    except UnicodeDecodeError:\n",
        "        # Si falla, probamos con latin1 que es más permisiva\n",
        "        df = pd.read_csv(file_path, header=None, encoding='latin1')\n",
        "\n",
        "    dataset_pairs = []\n",
        "    for _, row in df.iterrows():\n",
        "        if len(row) >= 2:\n",
        "            input = str(row[0]).strip().strip('\"')\n",
        "            target = str(row[1]).strip().strip('\"')\n",
        "            dataset_pairs.append((input, target))\n",
        "\n",
        "    print(f\"Se cargaron {len(dataset_pairs)} ejemplos del dataset\")\n",
        "    return dataset_pairs\n",
        "\n",
        "def load_or_train_model(model_dir='/content/drive/MyDrive/Colab Notebooks/best_model'):\n",
        "    # Verificar si existe un modelo guardado\n",
        "    if os.path.exists(model_dir):\n",
        "        print(f\"Cargando modelo guardado desde {model_dir}\")\n",
        "        tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
        "        model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
        "\n",
        "        # Cargar mejor pérdida previa\n",
        "        loss_history_file = os.path.join(model_dir, 'loss_history.txt')\n",
        "        if os.path.exists(loss_history_file):\n",
        "            with open(loss_history_file, 'r') as f:\n",
        "                best_loss = float(f.read().strip())\n",
        "            print(f\"Cargando mejor pérdida previa: {best_loss:.4f}\")\n",
        "            return model, tokenizer, best_loss > 0.04  # Solo entrenar si la pérdida es mayor que 0.04\n",
        "    else:\n",
        "        print(\"No se encontró modelo guardado. Iniciando con t5-small base\")\n",
        "        tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "        model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "    # Si llegamos aquí, se requiere entrenamiento\n",
        "    return model, tokenizer, True\n",
        "\n",
        "def train_model(model, tokenizer, dataset_pairs, model_dir='/content/drive/MyDrive/Colab Notebooks/best_model'):\n",
        "    # Mover modelo a GPU si está disponible\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Crear dataset y dataloader con batch_size optimizado\n",
        "    batch_size = 16 if torch.cuda.is_available() else 8\n",
        "    dataset = CodeTranslationDataset(dataset_pairs, tokenizer, max_length=128)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Configurar optimizador con un learning rate apropiado\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
        "\n",
        "    # Scheduler más adecuado para este tipo de tarea\n",
        "    num_epochs = 10\n",
        "    total_steps = len(dataloader) * num_epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * total_steps),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Cargar mejor pérdida previa si existe\n",
        "    best_loss = float('inf')\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    loss_history_file = os.path.join(model_dir, 'loss_history.txt')\n",
        "    if os.path.exists(loss_history_file):\n",
        "        with open(loss_history_file, 'r') as f:\n",
        "            best_loss = float(f.read().strip())\n",
        "        print(f\"Cargando mejor pérdida previa: {best_loss:.4f}\")\n",
        "\n",
        "    # Variable para guardar solo cada 2 épocas o cuando haya mejora significativa\n",
        "    save_epoch_interval = 2\n",
        "    min_improvement = 0.01  # 1% de mejora para guardar el modelo\n",
        "\n",
        "    # Entrenamiento\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        # Usar tqdm para mostrar progreso si está disponible\n",
        "        try:\n",
        "            from tqdm import tqdm\n",
        "            iterator = tqdm(dataloader, desc=f\"Época {epoch+1}/{num_epochs}\")\n",
        "        except ImportError:\n",
        "            iterator = dataloader\n",
        "\n",
        "        for batch in iterator:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mover batch a GPU si está disponible\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Habilitar mixed precision si está disponible\n",
        "            with torch.cuda.amp.autocast() if torch.cuda.is_available() else contextlib.nullcontext():\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping para evitar explosión de gradientes\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_loss = total_loss/len(dataloader)\n",
        "        print(f\"Época {epoch+1}/{num_epochs}, Loss promedio: {avg_loss:.4f}\")\n",
        "\n",
        "        # Guardar el modelo solo si hay una mejora significativa o cada save_epoch_interval\n",
        "        should_save = False\n",
        "\n",
        "        if avg_loss < best_loss - min_improvement:\n",
        "            print(f\"Mejora significativa: {best_loss:.4f} -> {avg_loss:.4f}\")\n",
        "            should_save = True\n",
        "        elif (epoch + 1) % save_epoch_interval == 0:\n",
        "            print(f\"Guardando por intervalo de época\")\n",
        "            should_save = True\n",
        "\n",
        "        if should_save:\n",
        "            best_loss = min(best_loss, avg_loss)\n",
        "            # Guardar en el directorio especificado\n",
        "            model.save_pretrained(model_dir)\n",
        "            tokenizer.save_pretrained(model_dir)\n",
        "\n",
        "            # Guardar la mejor pérdida\n",
        "            with open(loss_history_file, 'w') as f:\n",
        "                f.write(str(best_loss))\n",
        "\n",
        "            print(f\"Modelo guardado con loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Si alcanzamos una pérdida <= 0.04, terminamos el entrenamiento temprano\n",
        "        if avg_loss <= 0.04:\n",
        "            print(\"Se alcanzó la pérdida objetivo de 0.04 o menos. Terminando entrenamiento.\")\n",
        "            break\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def translate_code(model, tokenizer, input_code, device):\n",
        "    # Clean input code\n",
        "    input_code = input_code.strip()\n",
        "\n",
        "    # Tokenizar y mover a GPU de una vez\n",
        "    inputs = tokenizer(input_code, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
        "\n",
        "    # Configuración de generación más eficiente\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=2,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Decodificar resultado\n",
        "    translated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    translated_code = translated_code.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
        "\n",
        "    return translated_code\n",
        "\n",
        "def format_output_with_random_number(translated_code):\n",
        "    if \"Algoritmo conversion\" in translated_code and not \"conversion_\" in translated_code:\n",
        "        random_num = random.randint(1000, 9999)\n",
        "        translated_code = translated_code.replace(\"Algoritmo conversion\", f\"Algoritmo conversion_{random_num}\")\n",
        "    return translated_code\n",
        "\n",
        "def tokenize_code(code):\n",
        "    \"\"\"Tokeniza el código para evaluación de métricas de manera más adecuada para código fuente\"\"\"\n",
        "    # Reemplazar espacios en blanco alrededor de símbolos comunes para que sean tokens separados\n",
        "    for symbol in '(){}[]<>;:,.+-*/=&|^~!%':\n",
        "        code = code.replace(symbol, f' {symbol} ')\n",
        "\n",
        "    # Dividir por espacios en blanco y eliminar tokens vacíos\n",
        "    tokens = [token for token in re.split(r'\\s+', code) if token]\n",
        "    return tokens\n",
        "\n",
        "def evaluate_model_metrics(model, tokenizer, test_data, device):\n",
        "    \"\"\"Evalúa el modelo utilizando varias métricas comunes en NLP y específicas para código\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    print(\"\\n=== Evaluación de Métricas del Modelo ===\")\n",
        "\n",
        "    # Preparar listas para almacenar resultados\n",
        "    references_bleu = []\n",
        "    hypotheses_bleu = []\n",
        "    references_raw = []\n",
        "    hypotheses_raw = []\n",
        "    exact_match_count = 0\n",
        "\n",
        "    # Inicializar ROUGE para evaluación\n",
        "    try:\n",
        "        rouge = Rouge()\n",
        "        rouge_available = True\n",
        "    except:\n",
        "        print(\"ROUGE no está disponible, instalarlo con: pip install rouge\")\n",
        "        rouge_available = False\n",
        "\n",
        "    # Evaluar cada par en el conjunto de prueba\n",
        "    for input_code, reference_code in test_data:\n",
        "        # Traducir el código de entrada\n",
        "        translated_code = translate_code(model, tokenizer, input_code, device)\n",
        "\n",
        "        # Formatear si es necesario\n",
        "        translated_code = format_output_with_random_number(translated_code)\n",
        "\n",
        "        # Tokenizar para BLEU\n",
        "        reference_tokens = [tokenize_code(reference_code)]\n",
        "        hypothesis_tokens = tokenize_code(translated_code)\n",
        "\n",
        "        references_bleu.append(reference_tokens)\n",
        "        hypotheses_bleu.append(hypothesis_tokens)\n",
        "\n",
        "        # Almacenar versiones sin procesar para otras métricas\n",
        "        references_raw.append(reference_code)\n",
        "        hypotheses_raw.append(translated_code)\n",
        "\n",
        "        # Comprobar coincidencia exacta\n",
        "        if translated_code.strip() == reference_code.strip():\n",
        "            exact_match_count += 1\n",
        "\n",
        "    # Calcular métricas\n",
        "    # 1. BLEU Score\n",
        "    bleu_score = corpus_bleu(references_bleu, hypotheses_bleu)\n",
        "\n",
        "    # 2. Exact Match (precisión exacta)\n",
        "    exact_match_accuracy = exact_match_count / len(test_data) * 100\n",
        "\n",
        "    # 3. ROUGE (si está disponible)\n",
        "    if rouge_available and len(references_raw) > 0:\n",
        "        try:\n",
        "            rouge_scores = rouge.get_scores(hypotheses_raw, references_raw, avg=True)\n",
        "            rouge_l_f = rouge_scores['rouge-l']['f'] * 100\n",
        "        except Exception as e:\n",
        "            print(f\"Error al calcular ROUGE: {e}\")\n",
        "            rouge_l_f = 0\n",
        "    else:\n",
        "        rouge_l_f = 0\n",
        "\n",
        "    # 4. Codebleu - aproximación simplificada\n",
        "    # Aquí implementamos una métrica específica para código que evalúa palabras clave importantes\n",
        "    # como \"Algoritmo\", \"Proceso\", etc.\n",
        "\n",
        "    def calculate_keyword_match_score(refs, hyps, keywords):\n",
        "        \"\"\"Calcula el puntaje de coincidencia de palabras clave importantes en código\"\"\"\n",
        "        total_score = 0\n",
        "        for ref, hyp in zip(refs, hyps):\n",
        "            ref_contains = sum(1 for kw in keywords if kw.lower() in ref.lower())\n",
        "            hyp_contains = sum(1 for kw in keywords if kw.lower() in hyp.lower())\n",
        "\n",
        "            if ref_contains > 0:\n",
        "                score = hyp_contains / ref_contains\n",
        "                total_score += min(score, 1.0)\n",
        "            else:\n",
        "                total_score += 1.0 if hyp_contains == 0 else 0.0\n",
        "\n",
        "        return (total_score / len(refs)) * 100 if refs else 0\n",
        "\n",
        "    # Palabras clave importantes en el contexto de tu traducción de código\n",
        "    code_keywords = [\"Algoritmo\", \"Proceso\", \"FinAlgoritmo\", \"Escribir\", \"Leer\"]\n",
        "    keyword_score = calculate_keyword_match_score(references_raw, hypotheses_raw, code_keywords)\n",
        "\n",
        "    # 5. Estructura sintáctica - aproximación simple\n",
        "    def calculate_structure_score(refs, hyps):\n",
        "        \"\"\"Evalúa la similitud estructural basada en indentación y símbolos de estructura\"\"\"\n",
        "        structural_symbols = [\"Algoritmo\", \"Proceso\", \"FinAlgoritmo\", \"Escribir\", \"Leer\"]\n",
        "        return calculate_keyword_match_score(refs, hyps, structural_symbols)\n",
        "\n",
        "    structure_score = calculate_structure_score(references_raw, hypotheses_raw)\n",
        "\n",
        "    # Imprimir resultados de métricas\n",
        "    print(\"\\n=== Resultados de Evaluación ===\")\n",
        "    print(f\"1. BLEU Score: {bleu_score:.4f}\")\n",
        "    print(f\"2. Exact Match Accuracy: {exact_match_accuracy:.2f}%\")\n",
        "    print(f\"3. ROUGE-L F1: {rouge_l_f:.2f}%\")\n",
        "    print(f\"4. Keyword Match Score: {keyword_score:.2f}%\")\n",
        "    print(f\"5. Structure Similarity Score: {structure_score:.2f}%\")\n",
        "\n",
        "    # Calcular score compuesto ponderado\n",
        "    composite_score = (\n",
        "        0.3 * bleu_score * 100 +  # Ponderación de BLEU\n",
        "        0.2 * exact_match_accuracy +  # Ponderación de Exact Match\n",
        "        0.2 * rouge_l_f +  # Ponderación de ROUGE-L\n",
        "        0.15 * keyword_score +  # Ponderación de Keyword Match\n",
        "        0.15 * structure_score  # Ponderación de Structure Similarity\n",
        "    )\n",
        "\n",
        "    print(f\"\\nComposite Quality Score: {composite_score:.2f}%\")\n",
        "\n",
        "    # Guardar resultados en un archivo\n",
        "    metrics_results = {\n",
        "        'bleu': bleu_score,\n",
        "        'exact_match': exact_match_accuracy / 100,\n",
        "        'rouge_l': rouge_l_f / 100,\n",
        "        'keyword_score': keyword_score / 100,\n",
        "        'structure_score': structure_score / 100,\n",
        "        'composite_score': composite_score / 100\n",
        "    }\n",
        "\n",
        "    return metrics_results\n",
        "\n",
        "def create_test_dataset(file_path=None, sample_size=50):\n",
        "    \"\"\"Crea un conjunto de prueba separado para evaluación\"\"\"\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        # Cargar desde archivo si existe\n",
        "        all_data = load_dataset(file_path)\n",
        "\n",
        "        # Asegurar que tenemos suficientes ejemplos\n",
        "        if len(all_data) < sample_size:\n",
        "            print(f\"Advertencia: El dataset solo contiene {len(all_data)} ejemplos, se usarán todos.\")\n",
        "            return all_data\n",
        "\n",
        "        # Tomar una muestra aleatoria para test\n",
        "        test_indices = random.sample(range(len(all_data)), sample_size)\n",
        "        return [all_data[i] for i in test_indices]\n",
        "    else:\n",
        "        # Datos de ejemplo para pruebas si no hay archivo\n",
        "        return [\n",
        "            (\"1 1 1 0 0 4 1 6 'tara' 1\", \"Algoritmo conversion\\nProceso\\n    Escribir tara\\nFinProceso\"),\n",
        "            (\"1 1 1 0 0 4 1 8 'comand' 1\", \"Algoritmo conversion\\nProceso\\n    Escribir comand\\nFinProceso\"),\n",
        "            (\"1 1 1 0 0 5 1 10 'variable' 1\", \"Algoritmo conversion\\nProceso\\n    Escribir variable\\nFinProceso\"),\n",
        "            (\"1 1 1 0 0 5 1 7 'count' 1\", \"Algoritmo conversion\\nProceso\\n    Escribir count\\nFinProceso\"),\n",
        "            # Agregar más ejemplos aquí...\n",
        "        ]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configurar para usar GPU si está disponible\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "    # Importación condicional para mixed precision\n",
        "    import contextlib\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"CUDA disponible, habilitando optimizaciones adicionales\")\n",
        "\n",
        "    # Cargar o inicializar modelo\n",
        "    model, tokenizer, needs_training = load_or_train_model()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Solo entrenar si es necesario\n",
        "    if needs_training:\n",
        "        print(\"Cargando dataset para entrenamiento...\")\n",
        "        dataset_pairs = load_dataset('dataset.csv')\n",
        "\n",
        "        print(\"Iniciando entrenamiento...\")\n",
        "        model, tokenizer = train_model(model, tokenizer, dataset_pairs)\n",
        "    else:\n",
        "        print(\"Omitiendo entrenamiento debido a que la pérdida ya es óptima\")\n",
        "\n",
        "    # Crear conjunto de test (independiente del conjunto de entrenamiento)\n",
        "    print(\"\\nPreparando conjunto de test para evaluación...\")\n",
        "    test_dataset = create_test_dataset('dataset.csv', sample_size=50)\n",
        "    print(f\"Se utilizarán {len(test_dataset)} ejemplos para evaluación\")\n",
        "\n",
        "    # Evaluar el modelo con métricas\n",
        "    print(\"\\nEvaluando métricas del modelo...\")\n",
        "    metrics = evaluate_model_metrics(model, tokenizer, test_dataset, device)\n",
        "\n",
        "    # Guardar métricas en un archivo\n",
        "    metrics_file = os.path.join('/content/drive/MyDrive/Colab Notebooks/best_model', 'metrics.txt')\n",
        "    with open(metrics_file, 'w') as f:\n",
        "        for metric_name, metric_value in metrics.items():\n",
        "            f.write(f\"{metric_name}: {metric_value:.6f}\\n\")\n",
        "    print(f\"Métricas guardadas en {metrics_file}\")\n",
        "\n",
        "    # Probar con ejemplos específicos\n",
        "    test_codes = [\n",
        "        \"1 1 1 0 0 4 1 6 'tara' 1\",\n",
        "        \"1 1 1 0 0 4 1 8 'comand' 1\",\n",
        "        \"1 1 1 0 0 5 1 10 'variable' 1\",\n",
        "        \"1 1 1 0 0 5 1 7 'count' 1\",\n",
        "    ]\n",
        "\n",
        "    print(\"\\nPruebas de traducción con ejemplos específicos:\")\n",
        "    for code in test_codes:\n",
        "        result = translate_code(model, tokenizer, code, device)\n",
        "        formatted_result = format_output_with_random_number(result)\n",
        "        print(f\"\\nEntrada: {code}\")\n",
        "        print(f\"Salida:\\n{formatted_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfo258rr3K6_",
        "outputId": "1f27c182-d4e4-4a29-beb6-96b15deaa0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cpu\n",
            "Cargando modelo guardado desde /content/drive/MyDrive/Colab Notebooks/best_model\n",
            "Cargando mejor pérdida previa: 0.0227\n",
            "Omitiendo entrenamiento debido a que la pérdida ya es óptima\n",
            "\n",
            "Preparando conjunto de test para evaluación...\n",
            "Se utilizarán 4 ejemplos para evaluación\n",
            "\n",
            "Evaluando métricas del modelo...\n",
            "\n",
            "=== Evaluación de Métricas del Modelo ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Resultados de Evaluación ===\n",
            "1. BLEU Score: 0.0000\n",
            "2. Exact Match Accuracy: 0.00%\n",
            "3. ROUGE-L F1: 36.36%\n",
            "4. Keyword Match Score: 66.67%\n",
            "5. Structure Similarity Score: 66.67%\n",
            "\n",
            "Composite Quality Score: 27.27%\n",
            "Métricas guardadas en /content/drive/MyDrive/Colab Notebooks/best_model/metrics.txt\n",
            "\n",
            "Pruebas de traducción con ejemplos específicos:\n",
            "\n",
            "Entrada: 1 1 1 0 0 4 1 6 'tara' 1\n",
            "Salida:\n",
            "Algoritmo conversion_1038 Escribir 'tara' FinAlgoritoma\n",
            "\n",
            "Entrada: 1 1 1 0 0 4 1 8 'comand' 1\n",
            "Salida:\n",
            "Algoritmo conversion_8682 Escribir 'comand' FinAlgoritomic\n",
            "\n",
            "Entrada: 1 1 1 0 0 5 1 10 'variable' 1\n",
            "Salida:\n",
            "Algoritmo conversion_4489 Escribir 'variable' FinAlgoritomic\n",
            "\n",
            "Entrada: 1 1 1 0 0 5 1 7 'count' 1\n",
            "Salida:\n",
            "Algoritmo conversion_2509 Escribir 'count' FinAlgoritoMo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creacion de la API con flask"
      ],
      "metadata": {
        "id": "-Ayd0X3w7ROo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con este codigo se levantara un servidor web en flask, con un endpoint \"/translate\" para hacer un un post de un archivo .dfd, y hacer uso del modelo previamente entrenado para la traduccion, por defecto corre en el puerto 5000, se generan dos accesos: uno local para acceder dentro de el entorno de google colab, y un tunel web para acceder desde otra maquina, el enlace del tunel se puede encontrar en la linea anterior a **\"Puedes acceder a la API usando el enlace superior o enviar solicitudes POST a /translate**\n",
        "#**importante: antes de ejecutar el servidor en flask asegurarse de haber cargado el template \"index.html\" en los archivos de google colab**\n"
      ],
      "metadata": {
        "id": "LgBWJw2-_Fei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "instalar cors para evitar posibles errores de acceso"
      ],
      "metadata": {
        "id": "_Oo5BbQ0AGe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-cors"
      ],
      "metadata": {
        "id": "Ca6j1lNsACKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify, send_file\n",
        "from flask_cors import CORS\n",
        "from werkzeug.utils import secure_filename\n",
        "import os\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import tempfile\n",
        "app = Flask(__name__)\n",
        "CORS(app, resources={r\"/*\": {\"origins\": [\"*\", \"null\"]}}, supports_credentials=True)\n",
        "@app.after_request\n",
        "def after_request(response):\n",
        "    # Permitir explícitamente el origen \"null\" para archivos HTML locales\n",
        "    origin = request.headers.get('Origin')\n",
        "    if origin == 'null' or origin is None:\n",
        "        response.headers.add('Access-Control-Allow-Origin', 'null')\n",
        "    else:\n",
        "        response.headers.add('Access-Control-Allow-Origin', origin)\n",
        "\n",
        "    response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')\n",
        "    response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')\n",
        "    response.headers.add('Access-Control-Allow-Credentials', 'true')\n",
        "    return response\n",
        "\n",
        "# Configuración básica\n",
        "UPLOAD_FOLDER = '/content/uploads'\n",
        "OUTPUT_FOLDER = '/content/outputs'\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# Cargar modelo y tokenizador (esto se ejecutará solo una vez al iniciar la API)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "\n",
        "def load_model():\n",
        "    global model, tokenizer\n",
        "    model_dir = '/content/drive/MyDrive/Colab Notebooks/best_model'\n",
        "    print(f\"Cargando modelo desde {model_dir}\")\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_dir).to(device)\n",
        "    print(\"Modelo cargado exitosamente\")\n",
        "\n",
        "load_model()\n",
        "\n",
        "def process_dfd_file(file_path):\n",
        "    \"\"\"Procesa el archivo .dfd según los requisitos\"\"\"\n",
        "    try:\n",
        "        # Leer el archivo .dfd como texto\n",
        "        with open(file_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # Eliminar la primera línea\n",
        "        if len(lines) > 0:\n",
        "            lines = lines[1:]\n",
        "\n",
        "        # Unir todas las líneas en una sola, eliminando saltos de línea y espacios extras\n",
        "        single_line = ' '.join(line.strip() for line in lines)\n",
        "\n",
        "        return single_line\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando archivo: {e}\")\n",
        "        return None\n",
        "\n",
        "def translate_code(input_code):\n",
        "    \"\"\"Traduce el código usando el modelo cargado\"\"\"\n",
        "    try:\n",
        "        # Limpiar el código de entrada\n",
        "        input_code = input_code.strip()\n",
        "\n",
        "        # Tokenizar y mover a GPU\n",
        "        inputs = tokenizer(input_code, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
        "\n",
        "        # Generar la traducción\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=128,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        # Decodificar el resultado\n",
        "        translated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        translated_code = translated_code.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
        "\n",
        "        return translated_code\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la traducción: {e}\")\n",
        "        return None\n",
        "\n",
        "@app.route('/translate', methods=['POST'])\n",
        "def translate_file():\n",
        "    \"\"\"Endpoint principal para la traducción de archivos\"\"\"\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No se proporcionó archivo'}), 400\n",
        "\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({'error': 'Nombre de archivo vacío'}), 400\n",
        "\n",
        "    # Verificar que el archivo tenga extensión .dfd\n",
        "    if not file.filename.lower().endswith('.dfd'):\n",
        "        return jsonify({'error': 'El archivo debe tener extensión .dfd'}), 400\n",
        "\n",
        "    try:\n",
        "        # Guardar el archivo temporalmente\n",
        "        filename = secure_filename(file.filename)\n",
        "        input_path = os.path.join(UPLOAD_FOLDER, filename)\n",
        "        file.save(input_path)\n",
        "\n",
        "        # Procesar el archivo .dfd\n",
        "        processed_code = process_dfd_file(input_path)\n",
        "        if processed_code is None:\n",
        "            return jsonify({'error': 'Error al procesar el archivo .dfd'}), 500\n",
        "\n",
        "        # Traducir el código\n",
        "        translated_code = translate_code(processed_code)\n",
        "        if translated_code is None:\n",
        "            return jsonify({'error': 'Error al traducir el código'}), 500\n",
        "\n",
        "        # Crear archivo de salida .txt\n",
        "        output_filename = os.path.splitext(filename)[0] + '_translated.txt'\n",
        "        output_path = os.path.join(OUTPUT_FOLDER, output_filename)\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            f.write(translated_code)\n",
        "\n",
        "        # Enviar el archivo de vuelta al cliente\n",
        "        return send_file(\n",
        "            output_path,\n",
        "            as_attachment=True,\n",
        "            download_name=output_filename,\n",
        "            mimetype='text/plain'\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': f'Error en el servidor: {str(e)}'}), 500\n",
        "    finally:\n",
        "        # Limpiar archivos temporales\n",
        "        if 'input_path' in locals() and os.path.exists(input_path):\n",
        "            os.remove(input_path)\n",
        "        # No eliminamos el output para que el cliente pueda descargarlo\n",
        "\n",
        "@app.route('/test', methods=['GET'])\n",
        "def test_api():\n",
        "    \"\"\"Endpoint de prueba para verificar que la API está funcionando\"\"\"\n",
        "    return jsonify({\n",
        "        'status': 'API funcionando',\n",
        "        'model_loaded': model is not None,\n",
        "        'device': str(device)\n",
        "    })\n",
        "\n",
        "def run_in_colab():\n",
        "    \"\"\"Función para ejecutar la API en Google Colab\"\"\"\n",
        "    from google.colab import output\n",
        "    import threading\n",
        "\n",
        "    # Iniciar Flask en un hilo separado\n",
        "    threading.Thread(target=app.run, kwargs={\n",
        "        'host': '0.0.0.0',\n",
        "        'port': 5000\n",
        "    }).start()\n",
        "\n",
        "    # Configurar el proxy en Colab\n",
        "    print(\"La API se está ejecutando en:\")\n",
        "    output.serve_kernel_port_as_window(5000)\n",
        "    print(\"Puedes acceder a la API usando el enlace superior o enviar solicitudes POST a /translate\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Si se ejecuta en Colab, usar run_in_colab()\n",
        "    # Si se ejecuta localmente, usar app.run()\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        # Montar Google Drive si es necesario\n",
        "        drive.mount('/content/drive')\n",
        "        run_in_colab()\n",
        "    except:\n",
        "        app.run(host='0.0.0.0', port=5000, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "nfGrUKe-sNMh",
        "outputId": "e1700f77-8b44-4e0d-e541-f11dc3b7a8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cpu\n",
            "Cargando modelo desde /content/drive/MyDrive/Colab Notebooks/best_model\n",
            "Modelo cargado exitosamente\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "La API se está ejecutando en:\n",
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(5000, \"/\", \"https://localhost:5000/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Puedes acceder a la API usando el enlace superior o enviar solicitudes POST a /translate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HpwcbHUd7May"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}